{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yKdAS6hOrZJc"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Imports and Setup\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Helper Functions (Activation and Loss)\n",
        "\n",
        "def sigmoid(A):\n",
        "    \"\"\"Sigmoid activation function.\"\"\"\n",
        "    # Ensure numerical stability\n",
        "    A = np.clip(A, -500, 500)\n",
        "    return 1 / (1 + np.exp(-A))\n",
        "\n",
        "def tanh(A):\n",
        "    \"\"\"Hyperbolic tangent activation function.\"\"\"\n",
        "    return np.tanh(A)\n",
        "\n",
        "def softmax(A):\n",
        "    \"\"\"Softmax activation function for the output layer.\"\"\"\n",
        "    # Subtract max for numerical stability\n",
        "    exp_A = np.exp(A - np.max(A, axis=1, keepdims=True))\n",
        "    sum_exp_A = np.sum(exp_A, axis=1, keepdims=True)\n",
        "    return exp_A / sum_exp_A\n",
        "\n",
        "def tanh_grad(A):\n",
        "    \"\"\"Gradient of the tanh function.\"\"\"\n",
        "    return 1 - np.tanh(A)**2\n",
        "\n",
        "def sigmoid_grad(A):\n",
        "    \"\"\"Gradient of the sigmoid function.\"\"\"\n",
        "    Z = sigmoid(A)\n",
        "    return Z * (1 - Z)\n",
        "\n",
        "def cross_entropy_error(y, Z3, batch_size):\n",
        "    \"\"\"Calculates the cross-entropy loss (Problem 3).\"\"\"\n",
        "    epsilon = 1e-7\n",
        "    # y is the one-hot true label\n",
        "    # L = -1/nb * sum(y * log(Z3))\n",
        "    return -np.sum(y * np.log(Z3 + epsilon)) / batch_size"
      ],
      "metadata": {
        "id": "RGNrv6S3rds4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: GetMiniBatch Class\n",
        "\n",
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    Iterator to get a mini-batch (provided in the prompt)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, X, y, batch_size=20, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self._X = X[shuffle_index]\n",
        "        self._y = y[shuffle_index]\n",
        "        # Use integer division for stop calculation\n",
        "        self._stop = (X.shape[0] + self.batch_size - 1) // self.batch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        p0 = item * self.batch_size\n",
        "        p1 = item * self.batch_size + self.batch_size\n",
        "        return self._X[p0:p1], self._y[p0:p1]\n",
        "\n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter * self.batch_size\n",
        "        p1 = self._counter * self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self._X[p0:p1], self._y[p0:p1]"
      ],
      "metadata": {
        "id": "IZdgJ8llrhBk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: ScratchSimpleNeuralNetrowkClassifier (Problems 1, 2, 4, 5, 6, 7)\n",
        "\n",
        "class ScratchSimpleNeuralNetrowkClassifier:\n",
        "    \"\"\"\n",
        "    Simple three-layer neural network classifier using tanh/sigmoid and SGD.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_features=784, n_nodes1=400, n_nodes2=200, n_output=10,\n",
        "                 sigma=0.01, lr=0.01, n_epoch=10, batch_size=20,\n",
        "                 activation_func=tanh, verbose=True):\n",
        "\n",
        "        self.verbose = verbose\n",
        "        self.n_features = n_features\n",
        "        self.n_nodes1 = n_nodes1\n",
        "        self.n_nodes2 = n_nodes2\n",
        "        self.n_output = n_output\n",
        "        self.sigma = sigma\n",
        "        self.lr = lr\n",
        "        self.n_epoch = n_epoch\n",
        "        self.batch_size = batch_size\n",
        "        self.activation_func = activation_func\n",
        "\n",
        "        # Select the correct gradient for backprop\n",
        "        self.activation_grad = tanh_grad if activation_func == tanh else sigmoid_grad\n",
        "\n",
        "        # Loss history for plotting (Problem 7)\n",
        "        self.loss = []\n",
        "        self.val_loss = []\n",
        "\n",
        "        # Initialize weights and biases (Problem 1)\n",
        "        self.W1 = self.sigma * np.random.randn(self.n_features, self.n_nodes1)\n",
        "        self.B1 = self.sigma * np.random.randn(self.n_nodes1)\n",
        "        self.W2 = self.sigma * np.random.randn(self.n_nodes1, self.n_nodes2)\n",
        "        self.B2 = self.sigma * np.random.randn(self.n_nodes2)\n",
        "        self.W3 = self.sigma * np.random.randn(self.n_nodes2, self.n_output)\n",
        "        self.B3 = self.sigma * np.random.randn(self.n_output)\n",
        "\n",
        "\n",
        "    def _forward(self, X):\n",
        "        \"\"\"Performs forward propagation (Problem 2).\"\"\"\n",
        "\n",
        "        # Layer 1\n",
        "        A1 = X @ self.W1 + self.B1\n",
        "        Z1 = self.activation_func(A1)\n",
        "\n",
        "        # Layer 2\n",
        "        A2 = Z1 @ self.W2 + self.B2\n",
        "        Z2 = self.activation_func(A2)\n",
        "\n",
        "        # Layer 3 (Output Layer)\n",
        "        A3 = Z2 @ self.W3 + self.B3\n",
        "        Z3 = softmax(A3)\n",
        "\n",
        "        return Z3, A3, Z2, A2, Z1, A1\n",
        "\n",
        "\n",
        "    def _backward_and_update(self, X, Y, Z3, Z2, A2, Z1, A1):\n",
        "        \"\"\"Performs back-propagation and updates parameters (Problem 4).\"\"\"\n",
        "\n",
        "        batch_size = X.shape[0]\n",
        "\n",
        "        # --- 3rd Layer ---\n",
        "        grad_A3 = (Z3 - Y) / batch_size\n",
        "        grad_W3 = Z2.T @ grad_A3\n",
        "        grad_B3 = np.sum(grad_A3, axis=0)\n",
        "        grad_Z2 = grad_A3 @ self.W3.T\n",
        "\n",
        "        # --- 2nd Layer ---\n",
        "        grad_A2 = grad_Z2 * self.activation_grad(A2)\n",
        "        grad_W2 = Z1.T @ grad_A2\n",
        "        grad_B2 = np.sum(grad_A2, axis=0)\n",
        "        grad_Z1 = grad_A2 @ self.W2.T\n",
        "\n",
        "        # --- 1st Layer ---\n",
        "        grad_A1 = grad_Z1 * self.activation_grad(A1)\n",
        "        grad_W1 = X.T @ grad_A1\n",
        "        grad_B1 = np.sum(grad_A1, axis=0)\n",
        "\n",
        "        # Update weights and biases (SGD)\n",
        "        self.W1 -= self.lr * grad_W1\n",
        "        self.B1 -= self.lr * grad_B1\n",
        "        self.W2 -= self.lr * grad_W2\n",
        "        self.B2 -= self.lr * grad_B2\n",
        "        self.W3 -= self.lr * grad_W3\n",
        "        self.B3 -= self.lr * grad_B3\n",
        "\n",
        "\n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        \"\"\"Learns the neural network classifier (Problem 6).\"\"\"\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"Starting training...\")\n",
        "            print(\"Epoch | Train Loss | Val Loss | Train Acc | Val Acc\")\n",
        "            print(\"-\" * 51)\n",
        "\n",
        "        for epoch in range(1, self.n_epoch + 1):\n",
        "            epoch_loss_sum = 0\n",
        "\n",
        "            # Use mini-batch iterator\n",
        "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size)\n",
        "\n",
        "            for mini_X_train, mini_y_train in get_mini_batch:\n",
        "\n",
        "                # Forward Propagation\n",
        "                Z3, A3, Z2, A2, Z1, A1 = self._forward(mini_X_train)\n",
        "\n",
        "                # Calculate loss (sum over the batch, scaled for the number of samples in the batch)\n",
        "                current_batch_size = mini_X_train.shape[0]\n",
        "                batch_loss = cross_entropy_error(mini_y_train, Z3, current_batch_size)\n",
        "                epoch_loss_sum += batch_loss * current_batch_size # Accumulate total loss sum\n",
        "\n",
        "                # Back-propagation and update\n",
        "                self._backward_and_update(mini_X_train, mini_y_train, Z3, Z2, A2, Z1, A1)\n",
        "\n",
        "            # Average training loss for the epoch\n",
        "            avg_train_loss = epoch_loss_sum / len(X)\n",
        "            self.loss.append(avg_train_loss)\n",
        "\n",
        "            # Validation and Accuracy\n",
        "            if X_val is not None and y_val is not None:\n",
        "                # Validation Loss\n",
        "                Z3_val, *_ = self._forward(X_val)\n",
        "                val_loss = cross_entropy_error(y_val, Z3_val, len(X_val))\n",
        "                self.val_loss.append(val_loss)\n",
        "\n",
        "                # Accuracy (requires converting one-hot to single-label)\n",
        "                y_pred_train = self.predict(X)\n",
        "                y_pred_val = self.predict(X_val)\n",
        "\n",
        "                y_single_train = np.argmax(y, axis=1)\n",
        "                y_single_val = np.argmax(y_val, axis=1)\n",
        "\n",
        "                train_acc = accuracy_score(y_single_train, y_pred_train)\n",
        "                val_acc = accuracy_score(y_single_val, y_pred_val)\n",
        "\n",
        "                if self.verbose:\n",
        "                    print(f\"{epoch:5} | {avg_train_loss:.6f} | {val_loss:.6f} | {train_acc:.4f} | {val_acc:.4f}\")\n",
        "            elif self.verbose:\n",
        "                print(f\"{epoch:5} | {avg_train_loss:.6f}\")\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Estimates class labels (Problem 5).\"\"\"\n",
        "        Z3, *_ = self._forward(X)\n",
        "        # Find the index of the highest probability\n",
        "        y_pred = np.argmax(Z3, axis=1)\n",
        "        return y_pred"
      ],
      "metadata": {
        "id": "dJ07k5XorjF8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n"
      ],
      "metadata": {
        "id": "kdiyd7PsrlPl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b25f32b-e86f-45c5-b147-817cff6b04ca"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"X_train = X_train.reshape(-1, 784)\n",
        "X_test = X_test.reshape(-1, 784)\"\"\""
      ],
      "metadata": {
        "id": "zQRxy5p7sDU2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "db6d57eb-a86a-4b73-9daa-747211736714"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'X_train = X_train.reshape(-1, 784)\\nX_test = X_test.reshape(-1, 784)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"X_train = X_train.astype(np.float64)\n",
        "X_test = X_test.astype(np.float64)\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "print(X_train.max())  # 1.0\n",
        "print(X_train.min())  # 0.\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "v-zsYlZYsL05",
        "outputId": "bdd9c147-c7b1-4038-9a32-54d77a6a91ec"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'X_train = X_train.astype(np.float64)\\nX_test = X_test.astype(np.float64)\\nX_train /= 255\\nX_test /= 255\\nprint(X_train.max())  # 1.0\\nprint(X_train.min())  # 0.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# --- Data Preprocessing Steps ---\n",
        "\n",
        "# 1. Combine training and testing data for a balanced 60/20/20 split\n",
        "X_all = np.concatenate([X_train, X_test], axis=0)\n",
        "y_int_all = np.concatenate([y_train, y_test], axis=0)\n",
        "\n",
        "# 2. Flatten Features (28x28 -> 784) and Normalize (0-255 -> 0-1)\n",
        "# Convert to float and divide by 255\n",
        "X_all = X_all.reshape(-1, 784).astype(float) / 255.0\n",
        "\n",
        "# 3. One-Hot Encoding (OHE) for labels\n",
        "# The loss function (cross-entropy) and output layer require OHE labels.\n",
        "enc = OneHotEncoder(sparse_output=False, categories='auto')\n",
        "y_one_hot_all = enc.fit_transform(y_int_all.reshape(-1, 1))\n",
        "\n",
        "# 4. Data Splitting: Train (60%), Validation (20%), Test (20%)\n",
        "# First split: Train (60%) vs. Temp (40%)\n",
        "X_train, X_temp, y_train_ohe, y_temp_ohe = train_test_split(\n",
        "    X_all, y_one_hot_all, test_size=0.4, random_state=42\n",
        ")\n",
        "\n",
        "# Second split: Validation (50% of Temp) vs. Test (50% of Temp) -> 20% each\n",
        "X_val, X_test, y_val_ohe, y_test_ohe = train_test_split(\n",
        "    X_temp, y_temp_ohe, test_size=0.5, random_state=42\n",
        ")\n",
        "\n",
        "# Rename OHE labels for compatibility with the ScratchSimpleNeuralNetrowkClassifier.fit()\n",
        "y_train = y_train_ohe\n",
        "y_val = y_val_ohe\n",
        "\n",
        "# --- Set Global Parameters ---\n",
        "N_FEATURES = X_train.shape[1]\n",
        "N_OUTPUT = y_train.shape[1]\n",
        "\n",
        "print(\"--- MNIST Data Preprocessing Complete ---\")\n",
        "print(f\"N_FEATURES: {N_FEATURES}, N_OUTPUT: {N_OUTPUT}\")\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AteXzMqQsOYj",
        "outputId": "10b3afa3-1309-465c-cf29-30ad4be7a126"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- MNIST Data Preprocessing Complete ---\n",
            "N_FEATURES: 784, N_OUTPUT: 10\n",
            "X_train shape: (42000, 784), y_train shape: (42000, 10)\n",
            "X_val shape: (14000, 784), y_val shape: (14000, 10)\n",
            "X_test shape: (14000, 784), y_test shape: (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Training and Evaluation (Problem 6)\n",
        "\n",
        "# Hyperparameters\n",
        "EPOCHS = 30\n",
        "LEARNING_RATE = 0.01\n",
        "BATCH_SIZE = 100\n",
        "\n",
        "# Initialize the classifier\n",
        "nn_classifier = ScratchSimpleNeuralNetrowkClassifier(\n",
        "    n_features=N_FEATURES, n_nodes1=400, n_nodes2=200, n_output=N_OUTPUT,\n",
        "    lr=LEARNING_RATE, n_epoch=EPOCHS, batch_size=BATCH_SIZE,\n",
        "    activation_func=tanh, verbose=True\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "nn_classifier.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "# Final Estimation on Validation Data\n",
        "y_pred_val = nn_classifier.predict(X_val)\n",
        "\n",
        "# Get true single labels for accuracy check\n",
        "y_val_single = np.argmax(y_val, axis=1)\n",
        "\n",
        "# Calculate Accuracy\n",
        "final_accuracy = accuracy_score(y_val_single, y_pred_val)\n",
        "print(f\"\\nFinal Validation Accuracy: {final_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwpaHuUyrx3Q",
        "outputId": "bb970360-b311-4d36-bc1d-d8db0e5a4642"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch | Train Loss | Val Loss | Train Acc | Val Acc\n",
            "---------------------------------------------------\n",
            "    1 | 2.295777 | 2.287416 | 0.3357 | 0.3343\n",
            "    2 | 2.263188 | 2.215753 | 0.4052 | 0.4034\n",
            "    3 | 1.978252 | 1.625892 | 0.4414 | 0.4427\n",
            "    4 | 1.314704 | 1.060700 | 0.6727 | 0.6683\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmRxbxGxscbe"
      },
      "source": [
        "X_train, X_val, y_train_one_hot, y_val_one_hot = train_test_split(X_train, y_train_one_hot, test_size=0.2, random_state=42)\n",
        "print(X_train.shape)\n",
        "print(X_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Plotting the Learning Curve (Problem 7)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, nn_classifier.n_epoch + 1), nn_classifier.loss, label='Training Loss')\n",
        "if nn_classifier.val_loss:\n",
        "    plt.plot(range(1, nn_classifier.n_epoch + 1), nn_classifier.val_loss, label='Validation Loss')\n",
        "\n",
        "plt.title('Learning Curve (Cross-Entropy Error)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (Cross-Entropy Error)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w0pVqLbkrzo_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}